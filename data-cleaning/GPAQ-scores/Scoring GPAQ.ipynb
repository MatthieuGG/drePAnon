{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11805e24",
   "metadata": {},
   "source": [
    "# Scoring GPAQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6a3702a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075f4a8a",
   "metadata": {},
   "source": [
    "### Import files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "659d421e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 - this needs to be equal to your .csv files amount\n"
     ]
    }
   ],
   "source": [
    "path = os.path.join(os.getcwd(),'sample')\n",
    "\n",
    "data_dict = {}\n",
    "\n",
    "for file_name in os.listdir(path):\n",
    "    if file_name.endswith('.csv'): #to only import .csv files and avoid any temporal file\n",
    "        file_path = os.path.join(path, file_name)\n",
    "        df = pd.read_csv(file_path).T #.T permits a transposition between lignes and columns\n",
    "        df = df.reset_index(drop = False) #otherwise 'ID' is used as index\n",
    "            \n",
    "        #rename columns' names according to first line\n",
    "        new_cols = df.iloc[0].tolist()  \n",
    "        df = df.rename(columns=dict(zip(df.columns, new_cols)))  \n",
    "        df = df.drop(df.index[0]) \n",
    "            \n",
    "        data_dict[file_name] = df\n",
    "\n",
    "print(f\"{len(data_dict)} - this needs to be equal to your .csv files amount\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4043212f",
   "metadata": {},
   "source": [
    "### Conditional checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a6aefd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "CHECK FOR DUPLICATES\n",
      "\n",
      "No duplicates in dataframe\n",
      "----------\n",
      "CHECK FOR EVERY YES OR NO ITEMS TO BE FILLED, AND CORRECTLY (1 or 2 value)\n",
      "\n",
      "----------\n",
      "CHECK FOR NO PA BEHAVIOR DESCRIBED IF NO PA MENTIONED BEFORE (NO)\n",
      "According to ONAPS, questionnaire must be deleated if PA described while NO mentioned\n",
      "\n",
      "----------\n",
      "CHECK FOR SOME PA BEHAVIOR DESCRIBED IF PA MENTIONED BEFORE (YES)\n",
      "\n",
      "----------\n",
      "CHECK FOR AT LEAST 1 MINUTE OF PA BEHAVIOR DESCRIBED IF PA MENTIONED BEFORE (YES)\n",
      "According to ONAPS, subdomain must be deleated if no PA described while YES mentioned\n",
      "\n",
      "----------\n",
      "CHECK FOR CORRECT TIME FORMAT: 7 days, 24 hours, 60 minutes\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"----------\")\n",
    "print('CHECK FOR DUPLICATES')\n",
    "print('')\n",
    "\n",
    "names_list = []\n",
    "\n",
    "for df_name, df in data_dict.items():\n",
    "    names_list.append(df_name)\n",
    "\n",
    "if len(names_list) == len(set(names_list)):\n",
    "    print(\"No duplicates in dataframe\")\n",
    "else:\n",
    "    print(\"Duplicates in dataframe\")\n",
    "    \n",
    "print(\"----------\")\n",
    "print('CHECK FOR EVERY YES OR NO ITEMS TO BE FILLED, AND CORRECTLY (1 or 2 value)')\n",
    "print('')\n",
    "\n",
    "columns_to_check = ['P1', 'P4', 'P7', 'P10', 'P13'] \n",
    "\n",
    "for key, df in data_dict.items():\n",
    "    mask = (df[columns_to_check] != 1) & (df[columns_to_check] != 2)\n",
    "    if mask.any().any():\n",
    "        print(f\"Dataframe {key} has invalid values (not 1 or 2) in the following columns:\")\n",
    "        print(df[mask][columns_to_check])\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "print(\"----------\")\n",
    "print('CHECK FOR NO PA BEHAVIOR DESCRIBED IF NO PA MENTIONED BEFORE (NO)')\n",
    "print('According to ONAPS, questionnaire must be deleated if PA described while NO mentioned')\n",
    "print('')\n",
    "\n",
    "for key, df in data_dict.items():\n",
    "    # check P1\n",
    "    mask = df['P1'] == 2\n",
    "    if mask.any():\n",
    "        sub_df = df.loc[mask, ['P1', 'P2', 'P3a', 'P3b']]\n",
    "        error_mask = sub_df[['P2', 'P3a', 'P3b']].notnull().any(axis=1)\n",
    "        if error_mask.any():\n",
    "            print(f\"Error in {key} for columns {sub_df.loc[error_mask].index.tolist()}\")\n",
    "            print(sub_df.loc[error_mask])\n",
    "\n",
    "    # check P4\n",
    "    mask = df['P4'] == 2\n",
    "    if mask.any():\n",
    "        sub_df = df.loc[mask, ['P4', 'P5', 'P6a', 'P6b']]\n",
    "        error_mask = sub_df[['P5', 'P6a', 'P6b']].notnull().any(axis=1)\n",
    "        if error_mask.any():\n",
    "            print(f\"Error in {key} for columns {sub_df.loc[error_mask].index.tolist()}\")\n",
    "            print(sub_df.loc[error_mask])\n",
    "    \n",
    "    # check P7\n",
    "    mask = df['P7'] == 2\n",
    "    if mask.any():\n",
    "        sub_df = df.loc[mask, ['P7', 'P8', 'P9a', 'P9b']]\n",
    "        error_mask = sub_df[['P8', 'P9a', 'P9b']].notnull().any(axis=1)\n",
    "        if error_mask.any():\n",
    "            print(f\"Error in {key} for columns {sub_df.loc[error_mask].index.tolist()}\")\n",
    "            print(sub_df.loc[error_mask]) \n",
    "\n",
    "    # check P10\n",
    "    mask = df['P10'] == 2\n",
    "    if mask.any():\n",
    "        sub_df = df.loc[mask, ['P10', 'P11', 'P12a', 'P12b']]\n",
    "        error_mask = sub_df[['P11', 'P12a', 'P12b']].notnull().any(axis=1)\n",
    "        if error_mask.any():\n",
    "            print(f\"Error in {key} for columns {sub_df.loc[error_mask].index.tolist()}\")\n",
    "            print(sub_df.loc[error_mask]) \n",
    "\n",
    "    # check P13\n",
    "    mask = df['P13'] == 2\n",
    "    if mask.any():\n",
    "        sub_df = df.loc[mask, ['P13', 'P14', 'P15a', 'P15b']]\n",
    "        error_mask = sub_df[['P14', 'P15a', 'P15b']].notnull().any(axis=1)\n",
    "        if error_mask.any():\n",
    "            print(f\"Error in {key} for columns {sub_df.loc[error_mask].index.tolist()}\")\n",
    "            print(sub_df.loc[error_mask])\n",
    "            \n",
    "print(\"----------\")\n",
    "print('CHECK FOR SOME PA BEHAVIOR DESCRIBED IF PA MENTIONED BEFORE (YES)')\n",
    "print('')\n",
    "\n",
    "for key, df in data_dict.items():\n",
    "    cols_to_check = ['P1', 'P4', 'P7', 'P10', 'P13']\n",
    "    for col in cols_to_check:\n",
    "        mask = df[col] == 1\n",
    "        if mask.any():\n",
    "            col_index = df.columns.get_loc(col)\n",
    "            next_col = df.columns[col_index+1]\n",
    "            sub_df = df.loc[mask, [col, next_col]]\n",
    "            error_mask = (sub_df[col].notnull()) & (sub_df[next_col].isna() | (sub_df[next_col] < 1))\n",
    "            if error_mask.any():\n",
    "                print(f\"Error in {key} for columns {sub_df.loc[error_mask].index.tolist()}\")\n",
    "                print(sub_df.loc[error_mask])\n",
    "                \n",
    "print(\"----------\")\n",
    "print('CHECK FOR AT LEAST 1 MINUTE OF PA BEHAVIOR DESCRIBED IF PA MENTIONED BEFORE (YES)')\n",
    "print('According to ONAPS, subdomain must be deleated if no PA described while YES mentioned')\n",
    "print('')\n",
    "\n",
    "cols_to_check = ['P1', 'P4', 'P7', 'P10', 'P13']\n",
    "\n",
    "for key, df in data_dict.items():\n",
    "    for col in cols_to_check:\n",
    "        col_index = df.columns.get_loc(col)\n",
    "        mask = df[col] == 1\n",
    "        if mask.any():\n",
    "            sub_df = df.loc[mask, [col, df.columns[col_index+2], df.columns[col_index+3]]]\n",
    "            error_mask = sub_df[[df.columns[col_index+2], df.columns[col_index+3]]].isna().all(axis=1)\n",
    "            if error_mask.any():\n",
    "                print(f\"Error in {key} for columns {col} at index {col_index+2} and {col_index+3}\")\n",
    "                print(sub_df.loc[error_mask])\n",
    "            else:\n",
    "                sub_df = sub_df.loc[~error_mask]\n",
    "                error_mask = (sub_df[df.columns[col_index+2]].isna() | sub_df[df.columns[col_index+2]] < 1) & (sub_df[df.columns[col_index+3]].isna() | sub_df[df.columns[col_index+3]] < 1)\n",
    "                if error_mask.any():\n",
    "                    print(f\"Error in {key} for columns {col} at index {col_index+2} and {col_index+3}\")\n",
    "                    print(sub_df.loc[error_mask])\n",
    "                    \n",
    "print(\"----------\")\n",
    "print('CHECK FOR CORRECT TIME FORMAT: 7 days, 24 hours, 60 minutes')\n",
    "print('')\n",
    "\n",
    "columns_to_check = ['P2', 'P5', 'P8', 'P11', 'P14',\n",
    "                    'P3a', 'P6a', 'P9a', 'P12a', 'P15a', 'P16a',\n",
    "                    'P3b', 'P6b', 'P9b', 'P12b', 'P15b', 'P16b']\n",
    "\n",
    "acceptable_ranges = {\n",
    "    'P2': (0, 7),\n",
    "    'P5': (0, 7),\n",
    "    'P8': (0, 7),\n",
    "    'P11': (0, 7),\n",
    "    'P14': (0, 7),     \n",
    "    'P3a': (0, 16), #max authorized according to ONAPS recommandations\n",
    "    'P6a': (0, 16),\n",
    "    'P9a': (0, 16),\n",
    "    'P12a': (0, 16),\n",
    "    'P15a': (0, 16),\n",
    "    'P16a': (0, 24), #sedentarity has no maximum (24 hours)\n",
    "    'P3b': (0, 60),\n",
    "    'P6b': (0, 60),\n",
    "    'P9b': (0, 60),\n",
    "    'P12b': (0, 60),\n",
    "    'P15b': (0, 60),\n",
    "    'P16b': (0, 60)\n",
    "}\n",
    "\n",
    "aberrant_data = {}\n",
    "\n",
    "for key, df in data_dict.items():\n",
    "    for index, row in df.iterrows():\n",
    "        for col in columns_to_check:\n",
    "            value = row[col]\n",
    "            if value < acceptable_ranges[col][0] or value > acceptable_ranges[col][1]:\n",
    "                if key not in aberrant_data:\n",
    "                    aberrant_data[key] = []\n",
    "                aberrant_data[key].append((index, col))\n",
    "                \n",
    "for key, values in aberrant_data.items():\n",
    "    print(f\"Dataframe {key}:\")\n",
    "    for index, col in values:\n",
    "        print(f\"Wrong value in {col} at {index}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e65a8487",
   "metadata": {},
   "source": [
    "#### MET/min/week calculation\n",
    "We based our calculation on the GPAQ guides and ONAPS recommandations\n",
    "- VPA_work : vigorous PA realised at work\n",
    "- MVPA_work : moderate PA realised at work\n",
    "- travel : PA realised during displacement (considered moderate)\n",
    "- VPA_hobbies : vigorous PA realised in hobbies\n",
    "- MVPA_hobbies : moderate PA realised in hobbies\n",
    "- sed : sedentary time\n",
    "- work = VPA_work + MVPA_work\n",
    "- hobbies = VPA_hobbies + MVPA_hobbies\n",
    "- VPA = VPA_work + VPA_hobbies\n",
    "- MVPA = MVPA_work + MVPA_hobbies + travel\n",
    "- PAtot = VPA + MVPA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2fe036a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/24/kprm3l051vg4589bhtmv1_mw0000gn/T/ipykernel_75754/2883378772.py:2: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df = df.fillna(0)\n",
      "/var/folders/24/kprm3l051vg4589bhtmv1_mw0000gn/T/ipykernel_75754/2883378772.py:2: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df = df.fillna(0)\n"
     ]
    }
   ],
   "source": [
    "for key, df in data_dict.items():\n",
    "    df = df.fillna(0)\n",
    "    df['VPA_work'] = 8*(df['P2'] * ((df['P3a'] * 60) + df['P3b']))\n",
    "    df['MVPA_work'] = 4*(df['P5'] * ((df['P6a'] * 60) + df['P6b']))\n",
    "    df['travel'] = 4*(df['P8'] * ((df['P9a'] * 60) + df['P9b']))\n",
    "    df['VPA_hobbies'] = 8*(df['P11'] * ((df['P12a'] * 60) + df['P12b']))\n",
    "    df['MVPA_hobbies'] = 4*(df['P14'] * ((df['P15a'] * 60) + df['P15b']))\n",
    "    df['sed'] = 7 * ((df['P16a'] * 60) + df['P16b'])\n",
    "    df['work'] = df['VPA_work'] + df['MVPA_work']\n",
    "    df['hobbies'] = df['VPA_hobbies']+df['MVPA_hobbies']\n",
    "    df['VPA'] = df['VPA_work'] + df['VPA_hobbies']\n",
    "    df['MVPA'] = df['MVPA_work'] + df['MVPA_hobbies'] + df['travel']\n",
    "    df['PAtot'] = df['VPA'] + df['MVPA']\n",
    "    data_dict[key] = df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5329e159",
   "metadata": {},
   "source": [
    "### Saving files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2319c666",
   "metadata": {},
   "outputs": [],
   "source": [
    "#independant files\n",
    "saving_path_ind = os.path.join(os.getcwd(),'results')\n",
    "\n",
    "for key, value in data_dict.items():\n",
    "    filename = os.path.join(saving_path_ind, f\"{key}\")\n",
    "    value.to_csv(filename)\n",
    "    \n",
    "#concatenated files (one unique dataframe)\n",
    "concatenated_df = pd.concat(data_dict.values(), axis=0)\n",
    "concatenated_df = concatenated_df.sort_values(by=['ID'])\n",
    "concatenated_filename = os.path.join(saving_path_ind, \"concatenated_data.csv\")\n",
    "concatenated_df.to_csv(concatenated_filename, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
